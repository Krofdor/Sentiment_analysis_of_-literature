{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "path = '/home/kirill/Загрузки/2_Master.txt'\n",
    "\n",
    "test_txt = open(path,encoding=\"utf8\")\n",
    "#mas_texts = test_txt.read().split(\"stopword\")\n",
    "#print(mas_texts)\n",
    "data = test_txt.read()\n",
    "title = data.split('\\n', 1)[0]\n",
    "data = data.split('\\n', 2)[1]\n",
    "print(title)\n",
    "print(data)\n",
    "test_txt.close()\n",
    "\n",
    "test_txt = open(path,encoding=\"utf8\")\n",
    "#for i in range(len(mas_texts)):\n",
    "#    f = open('/home/kirill/Загрузки/New'+str(i)+'.txt','w')\n",
    "#    f.write(mas_texts[i])\n",
    "#    f.close()\n",
    "\n",
    "##text = test_txt.read()\n",
    "##res = len(text.split())\n",
    "##print(res)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "Unused_chars=[':',',','.','-','\\n','?','!',')','(', '«', '»']\n",
    "POS = ['NOUN', 'ADJF', 'ADJS', 'COMP', 'VERB', 'INFN', 'PRTF', 'PRTS', 'GRND', 'ADVB', 'NPRO']\n",
    "normal_form_words = []\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "for line in test_txt:\n",
    "        l=line.split('.')\n",
    "        for sentense in l:\n",
    "            #print(preprocess_text(sentense))\n",
    "            new_sent = preprocess_text(sentense)\n",
    "            words = new_sent.split()\n",
    "            for i in range(len(words)):\n",
    "                #Добавление слова в список слов и избавление от лишних символов\n",
    "                temp = words[i]\n",
    "                for char in Unused_chars:\n",
    "                    if char in temp:\n",
    "                        temp=words[i].replace(char,'')\n",
    "                        \n",
    "                word_pos = morph.parse(temp)[0].tag.POS\n",
    "                if(word_pos in POS):\n",
    "                    normal_form_words.append(morph.parse(temp)[0].normal_form)\n",
    "\n",
    "test_txt.close()\n",
    "\n",
    "#Слова для анализа\n",
    "\n",
    "emotions = open('/home/kirill/dictionary.csv')\n",
    "\n",
    "df = pd.read_csv(emotions)\n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "\n",
    "pat = '|'.join(normal_form_words)\n",
    "\n",
    "new = df[df['Russian Translation (Google Translate)'].str.contains(pat)]\n",
    "new = new.reset_index(drop = True)\n",
    "print(new)    \n",
    "\n",
    "##########################################\n",
    "\n",
    "#Все вхождения\n",
    "df.rename(columns={'Russian Translation (Google Translate)': 'Russian'}, inplace=True)  \n",
    "df = df.sort_values(by = ['Russian'])\n",
    "\n",
    "pat = '|'.join(df['Russian'])\n",
    "\n",
    "list_of_words = []\n",
    "\n",
    "for each in normal_form_words:\n",
    "    if (each in pat):\n",
    "        list_of_words.append(each)\n",
    "        \n",
    "list_of_words.sort()\n",
    "print(list_of_words)\n",
    "\n",
    "df_list_of_words = pd.DataFrame(pd.np.empty((0, 11)))\n",
    "df_list_of_words.columns = df.columns\n",
    "df_list_of_words['Russian'] = list_of_words\n",
    "\n",
    "new_list = []\n",
    "\n",
    "for each in df_list_of_words.itertuples():\n",
    "    for row in df.itertuples():\n",
    "        if (each[1] == row[1]):\n",
    "            \n",
    "            new_list.append(list(row[1:12]))\n",
    "            \n",
    "words_with_emotions = pd.DataFrame(new_list)\n",
    "words_with_emotions.columns = df.columns     \n",
    "\n",
    "##########################################\n",
    "\n",
    "print(len(words_with_emotions))\n",
    "\n",
    "class Labeled_Words:\n",
    "    def __init__(self, words, targets, target_names):\n",
    "        self.words = words\n",
    "        self.targets = targets\n",
    "        self.target_names = target_names\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Number):\n",
    "            return self.number == other.number\n",
    "        return False\n",
    "\n",
    "##########################################\n",
    "\n",
    "list_of_Labeled_words = Labeled_Words([],[],[])\n",
    "list_of_Labeled_words.words = words_with_emotions['Russian']\n",
    "list_of_Labeled_words.targets = words_with_emotions['Positive']\n",
    "\n",
    "##########################################\n",
    "\n",
    "print(\"Точность на обучающей выборке:\\n\")\n",
    "\n",
    "docs_test = list_of_Labeled_words.words\n",
    "\n",
    "nb_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('mnb', MultinomialNB()),\n",
    "                    ])\n",
    "\n",
    "nb_clf = nb_clf.fit(list_of_Labeled_words.words, list_of_Labeled_words.targets)\n",
    "\n",
    "predicted = nb_clf.predict(docs_test)\n",
    "nb = np.mean(predicted == list_of_Labeled_words.targets)\n",
    "print(\"CV + TF-IDF + NB: \", nb) \n",
    "\n",
    "svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42))\n",
    "                    ])\n",
    "\n",
    "svm_clf = svm_clf.fit(list_of_Labeled_words.words, list_of_Labeled_words.targets)\n",
    "\n",
    "predicted = svm_clf.predict(docs_test)\n",
    "svm = np.mean(predicted == list_of_Labeled_words.targets)\n",
    "print(\"CV + TF-IDF + SVM: \", svm) \n",
    "\n",
    "knn_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('knn', KNeighborsClassifier(n_neighbors=5, p=2, algorithm='kd_tree'))\n",
    "                    ])\n",
    "\n",
    "knn_clf = knn_clf.fit(list_of_Labeled_words.words, list_of_Labeled_words.targets)\n",
    "\n",
    "predicted = knn_clf.predict(docs_test)\n",
    "knn = np.mean(predicted == list_of_Labeled_words.targets)\n",
    "print(\"CV + TF-IDF + k-NN: \", knn) \n",
    "\n",
    "print(\"Точность на тестовой выборке:\\n\")\n",
    "\n",
    "docs_test = words_with_emotions['Russian']\n",
    "\n",
    "predicted = nb_clf.predict(docs_test)\n",
    "nb = np.mean(predicted == list_of_Labeled_words.targets)\n",
    "print(\"CV + TF-IDF + NB: \", nb) \n",
    "\n",
    "predicted = svm_clf.predict(docs_test)\n",
    "svm = np.mean(predicted == list_of_Labeled_words.targets)\n",
    "print(\"CV + TF-IDF + SVM: \", svm) \n",
    "\n",
    "predicted = knn_clf.predict(docs_test)\n",
    "knn = np.mean(predicted == list_of_Labeled_words.targets)\n",
    "print(\"CV + TF-IDF + k-NN: \", knn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
